{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tabulate\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_user = pd.read_csv(\"csv/x_train_user.csv\")\n",
    "x_train_item = pd.read_csv(\"csv/x_train_item.csv\")\n",
    "y_train = pd.read_csv(\"csv/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# scale training data\n",
    "unscaled_train_item = x_train_item\n",
    "unscaled_train_user = x_train_user\n",
    "y_train_unscaled    = y_train\n",
    "\n",
    "scalerItem = StandardScaler()#scalerItem = StandardScaler()\n",
    "scalerItem.fit(x_train_item)#scalerItem.fit(item_train)\n",
    "x_train_item = scalerItem.transform(x_train_item)#item_train = scalerItem.transform(item_train)\n",
    "\n",
    "scalerUser = StandardScaler()#scalerUser = StandardScaler()\n",
    "scalerUser.fit(x_train_user)#scalerUser.fit(user_train)\n",
    "x_train_user = scalerUser.transform(x_train_user)#user_train = scalerUser.transform(user_train)\n",
    "\n",
    "scalerTarget = MinMaxScaler((-1, 1))#scalerTarget = MinMaxScaler((-1, 1))\n",
    "scalerTarget.fit(y_train.values.reshape(-1, 1))#scalerTarget.fit(y_train.reshape(-1, 1))\n",
    "y_train = scalerTarget.transform(y_train.values.reshape(-1, 1))#y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
    "#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(np.allclose(unscaled_train_item, scalerItem.inverse_transform(x_train_item)))\n",
    "print(np.allclose(unscaled_train_user, scalerUser.inverse_transform(x_train_user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie/item training data shape: (80668, 21)\n",
      "movie/item test data shape: (20167, 21)\n"
     ]
    }
   ],
   "source": [
    "x_train_item, item_test = train_test_split(x_train_item, train_size=0.80, shuffle=True, random_state=1)\n",
    "x_train_user, user_test = train_test_split(x_train_user, train_size=0.80, shuffle=True, random_state=1)\n",
    "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
    "print(f\"movie/item training data shape: {x_train_item.shape}\")\n",
    "print(f\"movie/item test data shape: {item_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user_features = 19\n",
    "num_item_features = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 21)]         0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 32)           42144       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 32)           42656       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize (TFOpLamb  (None, 32)          0           ['sequential[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_1 (TFOpLa  (None, 32)          0           ['sequential_1[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['tf.math.l2_normalize[0][0]',   \n",
      "                                                                  'tf.math.l2_normalize_1[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 84,800\n",
      "Trainable params: 84,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_outputs = 32\n",
    "tf.random.set_seed(1)\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ###     \n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs),\n",
    "  \n",
    "    ### END CODE HERE ###  \n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ###     \n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs),\n",
    "    \n",
    "  \n",
    "  \n",
    "    ### END CODE HERE ###  \n",
    "])\n",
    "\n",
    "# create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
    "\n",
    "# create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
    "\n",
    "# compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2521/2521 [==============================] - 9s 3ms/step - loss: nan\n",
      "Epoch 2/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 3/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 4/30\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: nan\n",
      "Epoch 5/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 6/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 7/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 8/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 9/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 10/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 11/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 12/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 13/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 14/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 15/30\n",
      "2521/2521 [==============================] - 10s 4ms/step - loss: nan\n",
      "Epoch 16/30\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: nan\n",
      "Epoch 17/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 18/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 19/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 20/30\n",
      "2521/2521 [==============================] - 9s 4ms/step - loss: nan\n",
      "Epoch 21/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 22/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 23/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 24/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 25/30\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: nan\n",
      "Epoch 26/30\n",
      "2521/2521 [==============================] - 7s 3ms/step - loss: nan\n",
      "Epoch 27/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 28/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 29/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n",
      "Epoch 30/30\n",
      "2521/2521 [==============================] - 8s 3ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e3e6946b90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "model.fit([x_train_user, x_train_item], y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631/631 [==============================] - 2s 2ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([user_test, item_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_id = 5000\n",
    "new_rating_ave = 0.0\n",
    "new_action = 0.0\n",
    "new_adventure = 5.0\n",
    "new_animation = 0.0\n",
    "new_childrens = 0.0\n",
    "new_comedy = 0.0\n",
    "new_crime = 0.0\n",
    "new_documentary = 0.0\n",
    "new_drama = 0.0\n",
    "new_fantasy = 5.0\n",
    "new_filmnoir = 0.0 ##\n",
    "new_horror = 0.0\n",
    "new_imax = 0.0 ##\n",
    "new_musical = 0.0 ##\n",
    "new_mystery = 0.0\n",
    "new_romance = 0.0\n",
    "new_scifi = 0.0\n",
    "new_thriller = 0.0\n",
    "new_western = 0.0 ##\n",
    "new_war = 0.0 ##\n",
    "new_rating_count = 3\n",
    "#user id,rating count,rating ave,Action,Adventure,Animation,Children,Comedy,Crime,Documentary,Drama,Fantasy,***film-noir**Horror,**imax**,***musical***Mystery,Romance,Sci-Fi,Thriller***war***,***western***\n",
    "\n",
    "user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
    "                      new_action, new_adventure, new_animation, new_childrens,\n",
    "                      new_comedy, new_crime, new_documentary,\n",
    "                      new_drama, new_fantasy, new_filmnoir, new_horror, new_imax, new_musical, new_mystery,\n",
    "                      new_romance, new_scifi, new_thriller, new_western, new_war]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_vecs(user_vec, num_items):\n",
    "    \"\"\" given a user vector return:\n",
    "        user predict maxtrix to match the size of item_vecs \"\"\"\n",
    "    user_vecs = np.tile(user_vec, (num_items, 1))\n",
    "    return user_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_vecs = genfromtxt('./csv/item_vecs.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arsla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\arsla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 22 features, but StandardScaler is expecting 21 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\BİTİRME\\GitHub\\movie_recommender\\model_train.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/B%C4%B0T%C4%B0RME/GitHub/movie_recommender/model_train.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# scale our user and item vectors\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/B%C4%B0T%C4%B0RME/GitHub/movie_recommender/model_train.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m suser_vecs \u001b[39m=\u001b[39m scalerUser\u001b[39m.\u001b[39mtransform(user_vecs)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/B%C4%B0T%C4%B0RME/GitHub/movie_recommender/model_train.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sitem_vecs \u001b[39m=\u001b[39m scalerItem\u001b[39m.\u001b[39;49mtransform(item_vecs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/B%C4%B0T%C4%B0RME/GitHub/movie_recommender/model_train.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# make a prediction\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/B%C4%B0T%C4%B0RME/GitHub/movie_recommender/model_train.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_p \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
      "File \u001b[1;32mc:\\Users\\arsla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\arsla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    989\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    991\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m--> 992\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    993\u001b[0m     X,\n\u001b[0;32m    994\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    995\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    996\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    997\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    998\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    999\u001b[0m )\n\u001b[0;32m   1001\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m   1002\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\arsla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:558\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    555\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 558\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    560\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\arsla\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:359\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 359\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    360\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 22 features, but StandardScaler is expecting 21 features as input."
     ]
    }
   ],
   "source": [
    "# generate and replicate the user vector to match the number movies in the data set.\n",
    "user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
    "\n",
    "# scale our user and item vectors\n",
    "suser_vecs = scalerUser.transform(user_vecs)\n",
    "sitem_vecs = scalerItem.transform(item_vecs)\n",
    "\n",
    "# make a prediction\n",
    "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
    "\n",
    "# unscale y prediction \n",
    "y_pu = scalerTarget.inverse_transform(y_p)\n",
    "\n",
    "# sort the results, highest prediction first\n",
    "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
    "sorted_ypu   = y_pu[sorted_index]\n",
    "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
    "\n",
    "print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100835, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(scalerItem.inverse_transform(x_train_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100835, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(scalerUser.inverse_transform(x_train_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100835, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(uncaled_train_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100835, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(unclaed_train_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
